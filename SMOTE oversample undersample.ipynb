{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Amandawei067/HW5-441B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ywSRtNI4s5h"
   },
   "source": [
    "# 0.) Import the Credit Card Fraud Data From CCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nsG1QV154GYZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KJQfo8mz43Kz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fraudTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "mKWSRv-q98wE",
    "outputId": "29838bae-3f83-4216-f0da-f7ea7ee6ee69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-21 12:14:25</td>\n",
       "      <td>2291163933867244</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>...</td>\n",
       "      <td>33.9659</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>1968-03-19</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1371816865</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-06-21 12:14:33</td>\n",
       "      <td>3573030041201292</td>\n",
       "      <td>fraud_Sporer-Keebler</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>29.84</td>\n",
       "      <td>Joanne</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>3638 Marsh Union</td>\n",
       "      <td>...</td>\n",
       "      <td>40.3207</td>\n",
       "      <td>-110.4360</td>\n",
       "      <td>302</td>\n",
       "      <td>Sales professional, IT</td>\n",
       "      <td>1990-01-17</td>\n",
       "      <td>324cc204407e99f51b0d6ca0055005e7</td>\n",
       "      <td>1371816873</td>\n",
       "      <td>39.450498</td>\n",
       "      <td>-109.960431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-06-21 12:14:53</td>\n",
       "      <td>3598215285024754</td>\n",
       "      <td>fraud_Swaniawski, Nitzsche and Welch</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>41.28</td>\n",
       "      <td>Ashley</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>F</td>\n",
       "      <td>9333 Valentine Point</td>\n",
       "      <td>...</td>\n",
       "      <td>40.6729</td>\n",
       "      <td>-73.5365</td>\n",
       "      <td>34496</td>\n",
       "      <td>Librarian, public</td>\n",
       "      <td>1970-10-21</td>\n",
       "      <td>c81755dbbbea9d5c77f094348a7579be</td>\n",
       "      <td>1371816893</td>\n",
       "      <td>40.495810</td>\n",
       "      <td>-74.196111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-06-21 12:15:15</td>\n",
       "      <td>3591919803438423</td>\n",
       "      <td>fraud_Haley Group</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>60.05</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Williams</td>\n",
       "      <td>M</td>\n",
       "      <td>32941 Krystal Mill Apt. 552</td>\n",
       "      <td>...</td>\n",
       "      <td>28.5697</td>\n",
       "      <td>-80.8191</td>\n",
       "      <td>54767</td>\n",
       "      <td>Set designer</td>\n",
       "      <td>1987-07-25</td>\n",
       "      <td>2159175b9efe66dc301f149d3d5abf8c</td>\n",
       "      <td>1371816915</td>\n",
       "      <td>28.812398</td>\n",
       "      <td>-80.883061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-06-21 12:15:17</td>\n",
       "      <td>3526826139003047</td>\n",
       "      <td>fraud_Johnston-Casper</td>\n",
       "      <td>travel</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Nathan</td>\n",
       "      <td>Massey</td>\n",
       "      <td>M</td>\n",
       "      <td>5783 Evan Roads Apt. 465</td>\n",
       "      <td>...</td>\n",
       "      <td>44.2529</td>\n",
       "      <td>-85.0170</td>\n",
       "      <td>1126</td>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>1955-07-06</td>\n",
       "      <td>57ff021bd3f328f8738bb535c302a31b</td>\n",
       "      <td>1371816917</td>\n",
       "      <td>44.959148</td>\n",
       "      <td>-85.884734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
       "0           0   2020-06-21 12:14:25  2291163933867244   \n",
       "1           1   2020-06-21 12:14:33  3573030041201292   \n",
       "2           2   2020-06-21 12:14:53  3598215285024754   \n",
       "3           3   2020-06-21 12:15:15  3591919803438423   \n",
       "4           4   2020-06-21 12:15:17  3526826139003047   \n",
       "\n",
       "                               merchant        category    amt   first  \\\n",
       "0                 fraud_Kirlin and Sons   personal_care   2.86    Jeff   \n",
       "1                  fraud_Sporer-Keebler   personal_care  29.84  Joanne   \n",
       "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness  41.28  Ashley   \n",
       "3                     fraud_Haley Group        misc_pos  60.05   Brian   \n",
       "4                 fraud_Johnston-Casper          travel   3.19  Nathan   \n",
       "\n",
       "       last gender                       street  ...      lat      long  \\\n",
       "0   Elliott      M            351 Darlene Green  ...  33.9659  -80.9355   \n",
       "1  Williams      F             3638 Marsh Union  ...  40.3207 -110.4360   \n",
       "2     Lopez      F         9333 Valentine Point  ...  40.6729  -73.5365   \n",
       "3  Williams      M  32941 Krystal Mill Apt. 552  ...  28.5697  -80.8191   \n",
       "4    Massey      M     5783 Evan Roads Apt. 465  ...  44.2529  -85.0170   \n",
       "\n",
       "   city_pop                     job         dob  \\\n",
       "0    333497     Mechanical engineer  1968-03-19   \n",
       "1       302  Sales professional, IT  1990-01-17   \n",
       "2     34496       Librarian, public  1970-10-21   \n",
       "3     54767            Set designer  1987-07-25   \n",
       "4      1126      Furniture designer  1955-07-06   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714   \n",
       "1  324cc204407e99f51b0d6ca0055005e7  1371816873  39.450498 -109.960431   \n",
       "2  c81755dbbbea9d5c77f094348a7579be  1371816893  40.495810  -74.196111   \n",
       "3  2159175b9efe66dc301f149d3d5abf8c  1371816915  28.812398  -80.883061   \n",
       "4  57ff021bd3f328f8738bb535c302a31b  1371816917  44.959148  -85.884734   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_YuJa4IFKda",
    "outputId": "7e387d76-2dd6-472c-d598-5994ef2b9fda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/2p2qsjgx4yd0cjtw95rvsjwh0000gn/T/ipykernel_6492/3988105319.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_select[\"trans_date_trans_time\"] = pd.to_datetime(df_select[\"trans_date_trans_time\"])\n",
      "/var/folders/l4/2p2qsjgx4yd0cjtw95rvsjwh0000gn/T/ipykernel_6492/3988105319.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_select[\"time_var\"] = [i.second for i in df_select[\"trans_date_trans_time\"]]\n"
     ]
    }
   ],
   "source": [
    "df_select = df[[\"trans_date_trans_time\", \"category\", \"amt\", \"city_pop\", \"is_fraud\"]]\n",
    "#转换成时间\n",
    "df_select[\"trans_date_trans_time\"] = pd.to_datetime(df_select[\"trans_date_trans_time\"])\n",
    "#提取了trans_date_trans_time列中每个时间戳的秒数，并创建了一个新列time_var\n",
    "df_select[\"time_var\"] = [i.second for i in df_select[\"trans_date_trans_time\"]]\n",
    "\n",
    "X = pd.get_dummies(df_select, [\"category\"]).drop([\"trans_date_trans_time\", \"is_fraud\"], axis = 1)\n",
    "y = df[[\"is_fraud\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VsnpGe9-B3p"
   },
   "source": [
    "# 1.) Use scikit learn preprocessing to split the data into 70/30 in out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1tpCDMW198ym"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FZvnpERK981d"
   },
   "outputs": [],
   "source": [
    "#test,train,hold out\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JvCzIdgO983i"
   },
   "outputs": [],
   "source": [
    "X_test, X_holdout, y_test, y_holdout = train_test_split(X_test, y_test, test_size = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f7APv9N3986a"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#train+fit_transform\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_holdout = scaler.transform(X_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbkpNPyN-Gnk"
   },
   "source": [
    "# 2.) Make three sets of training data (Oversample, Undersample and SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gTTVciVkqopH"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/combine/_smote_enn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imblearn/utils/_param_validation.py:908\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    909\u001b[0m     HasMethods,\n\u001b[1;32m    910\u001b[0m     Hidden,\n\u001b[1;32m    911\u001b[0m     Interval,\n\u001b[1;32m    912\u001b[0m     Options,\n\u001b[1;32m    913\u001b[0m     StrOptions,\n\u001b[1;32m    914\u001b[0m     _ArrayLikes,\n\u001b[1;32m    915\u001b[0m     _Booleans,\n\u001b[1;32m    916\u001b[0m     _Callables,\n\u001b[1;32m    917\u001b[0m     _CVObjects,\n\u001b[1;32m    918\u001b[0m     _InstancesOf,\n\u001b[1;32m    919\u001b[0m     _IterablesNotString,\n\u001b[1;32m    920\u001b[0m     _MissingValues,\n\u001b[1;32m    921\u001b[0m     _NoneConstraint,\n\u001b[1;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[1;32m    923\u001b[0m     _RandomStates,\n\u001b[1;32m    924\u001b[0m     _SparseMatrices,\n\u001b[1;32m    925\u001b[0m     _VerboseHelper,\n\u001b[1;32m    926\u001b[0m     make_constraint,\n\u001b[1;32m    927\u001b[0m     validate_params,\n\u001b[1;32m    928\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gG88uxbiV4lZ"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "over_X, over_y = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "under_X, under_y = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "smote = SMOTE()\n",
    "smote_X, smote_y = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIJx2jvD-KEI"
   },
   "source": [
    "# 3.) Train three logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QhVMq92zvz4s"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NvwomEoaGAgN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "over_log = LogisticRegression().fit(over_X, over_y)\n",
    "\n",
    "under_log = LogisticRegression().fit(under_X, under_y)\n",
    "\n",
    "smote_log = LogisticRegression().fit(smote_X, smote_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeurmqI4-OoC"
   },
   "source": [
    "# 4.) Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tpSsOC0xsKs",
    "outputId": "fee6e3f0-6c06-489e-90da-59237e609bca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178003311019938"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwq8KTmsXhFY",
    "outputId": "7c01f959-58d8-44c6-e955-39fe84168d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9108064013052136"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjgQ8BQM99WR",
    "outputId": "a4f3fa30-e8f3-43f4-b562-959b200488e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178963026944025"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "IF_H74Ht-RIL"
   },
   "outputs": [],
   "source": [
    "# We see SMOTE performing with higher accuracy but is ACCURACY really the best measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6isBwtmL-R4p"
   },
   "source": [
    "# 5.) Which performed best in Out of Sample metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9fwAhujT-RN4"
   },
   "outputs": [],
   "source": [
    "# Sensitivity here in credit fraud is more important as seen from last class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "POIuy3rH-RQv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e42UoedMK6eq"
   },
   "outputs": [],
   "source": [
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mc_RyrHK6hX",
    "outputId": "408bb243-3c83-4337-97f5-21f57718360f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76273,  6772],\n",
       "       [   80,   233]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = over_log.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xT6gNjLuK6jS",
    "outputId": "2ed847ca-c7a5-414b-d45a-2e7acadfc9d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over Sample Sensitivity :  0.744408945686901\n"
     ]
    }
   ],
   "source": [
    "print(\"Over Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTHhw9P1K6lY",
    "outputId": "cadf0e8b-2b60-4ac8-fa81-275d3fab8c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75689,  7356],\n",
       "       [   79,   234]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = under_log.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g14fgEkT-RTV",
    "outputId": "72b0902b-c2f5-46c7-c49c-83126c75c94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Sample Sensitivity :  0.7476038338658147\n"
     ]
    }
   ],
   "source": [
    "print(\"Under Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iZ217d8LAR0",
    "outputId": "6d498a8f-bdd9-445d-97cc-6e4eec82574b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76281,  6764],\n",
       "       [   80,   233]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = smote_log.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5bfByOALAUk",
    "outputId": "a7071ed7-f896-4825-90f4-612a0603697a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Sample Sensitivity :  0.744408945686901\n"
     ]
    }
   ],
   "source": [
    "print(\"SMOTE Sample Sensitivity : \", cm[1,1] /( cm[1,0] + cm[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQa3sanl-XUk"
   },
   "source": [
    "# 6.) Pick two features and plot the two classes before and after SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MDSBmS_usbeJ"
   },
   "outputs": [],
   "source": [
    "raw_temp = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9QA-y6HCslBR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.214237</td>\n",
       "      <td>-0.285189</td>\n",
       "      <td>1.012773</td>\n",
       "      <td>-0.278751</td>\n",
       "      <td>-0.274627</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.191212</td>\n",
       "      <td>-0.322397</td>\n",
       "      <td>-0.265826</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>3.229577</td>\n",
       "      <td>-0.22789</td>\n",
       "      <td>-0.257957</td>\n",
       "      <td>-0.275508</td>\n",
       "      <td>-0.285052</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.445697</td>\n",
       "      <td>-0.287971</td>\n",
       "      <td>-1.068059</td>\n",
       "      <td>-0.278751</td>\n",
       "      <td>3.641307</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.191212</td>\n",
       "      <td>-0.322397</td>\n",
       "      <td>-0.265826</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>-0.309638</td>\n",
       "      <td>-0.22789</td>\n",
       "      <td>-0.257957</td>\n",
       "      <td>-0.275508</td>\n",
       "      <td>-0.285052</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.165386</td>\n",
       "      <td>-0.239943</td>\n",
       "      <td>1.301778</td>\n",
       "      <td>-0.278751</td>\n",
       "      <td>3.641307</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.191212</td>\n",
       "      <td>-0.322397</td>\n",
       "      <td>-0.265826</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>-0.309638</td>\n",
       "      <td>-0.22789</td>\n",
       "      <td>-0.257957</td>\n",
       "      <td>-0.275508</td>\n",
       "      <td>-0.285052</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.045558</td>\n",
       "      <td>-0.282848</td>\n",
       "      <td>-0.721254</td>\n",
       "      <td>-0.278751</td>\n",
       "      <td>-0.274627</td>\n",
       "      <td>2.970948</td>\n",
       "      <td>-0.191212</td>\n",
       "      <td>-0.322397</td>\n",
       "      <td>-0.265826</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>-0.309638</td>\n",
       "      <td>-0.22789</td>\n",
       "      <td>-0.257957</td>\n",
       "      <td>-0.275508</td>\n",
       "      <td>-0.285052</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028485</td>\n",
       "      <td>-0.291711</td>\n",
       "      <td>-0.374448</td>\n",
       "      <td>-0.278751</td>\n",
       "      <td>-0.274627</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.191212</td>\n",
       "      <td>3.101766</td>\n",
       "      <td>-0.265826</td>\n",
       "      <td>-0.322066</td>\n",
       "      <td>-0.309638</td>\n",
       "      <td>-0.22789</td>\n",
       "      <td>-0.257957</td>\n",
       "      <td>-0.275508</td>\n",
       "      <td>-0.285052</td>\n",
       "      <td>-0.314805</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458779</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418131</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505564 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.214237 -0.285189  1.012773 -0.278751 -0.274627 -0.336593 -0.191212   \n",
       "1      -0.445697 -0.287971 -1.068059 -0.278751  3.641307 -0.336593 -0.191212   \n",
       "2      -0.165386 -0.239943  1.301778 -0.278751  3.641307 -0.336593 -0.191212   \n",
       "3      -0.045558 -0.282848 -0.721254 -0.278751 -0.274627  2.970948 -0.191212   \n",
       "4       0.028485 -0.291711 -0.374448 -0.278751 -0.274627 -0.336593 -0.191212   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "458779       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "431549       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "418131       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "499572       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "440020       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "               7         8         9        10       11        12        13  \\\n",
       "0      -0.322397 -0.265826 -0.322066  3.229577 -0.22789 -0.257957 -0.275508   \n",
       "1      -0.322397 -0.265826 -0.322066 -0.309638 -0.22789 -0.257957 -0.275508   \n",
       "2      -0.322397 -0.265826 -0.322066 -0.309638 -0.22789 -0.257957 -0.275508   \n",
       "3      -0.322397 -0.265826 -0.322066 -0.309638 -0.22789 -0.257957 -0.275508   \n",
       "4       3.101766 -0.265826 -0.322066 -0.309638 -0.22789 -0.257957 -0.275508   \n",
       "...          ...       ...       ...       ...      ...       ...       ...   \n",
       "458779       NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
       "431549       NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
       "418131       NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
       "499572       NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
       "440020       NaN       NaN       NaN       NaN      NaN       NaN       NaN   \n",
       "\n",
       "              14        15        16  is_fraud  \n",
       "0      -0.285052 -0.314805 -0.180311       0.0  \n",
       "1      -0.285052 -0.314805 -0.180311       NaN  \n",
       "2      -0.285052 -0.314805 -0.180311       0.0  \n",
       "3      -0.285052 -0.314805 -0.180311       NaN  \n",
       "4      -0.285052 -0.314805 -0.180311       0.0  \n",
       "...          ...       ...       ...       ...  \n",
       "458779       NaN       NaN       NaN       0.0  \n",
       "431549       NaN       NaN       NaN       0.0  \n",
       "418131       NaN       NaN       NaN       0.0  \n",
       "499572       NaN       NaN       NaN       0.0  \n",
       "440020       NaN       NaN       NaN       0.0  \n",
       "\n",
       "[505564 rows x 18 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVlggv1lMvt2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjt1pnzgZcuO"
   },
   "source": [
    "# 7.) We want to compare oversampling, Undersampling and SMOTE across our 3 models (Logistic Regression, Logistic Regression Lasso and Decision Trees).\n",
    "\n",
    "# Make a dataframe that has a dual index and 9 Rows.\n",
    "# Calculate: Sensitivity, Specificity, Precision, Recall and F1 score. for out of sample data.\n",
    "# Notice any patterns across perfomance for this model. Does one totally out perform the others IE. over/under/smote or does a model perform better DT, Lasso, LR?\n",
    "# Choose what you think is the best model and why. test on Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0tIC3Nd1bx-N"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评价分类任务完成度的函数\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)  # Recall is the same as sensitivity\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return sensitivity, specificity, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/weijiayu/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Sensitivity Specificity  \\\n",
      "Model                     Sampling Technique                           \n",
      "Logistic Regression       Oversampling          0.712934    0.919618   \n",
      "                          Undersampling         0.712934    0.918679   \n",
      "                          SMOTE                 0.712934    0.916884   \n",
      "Logistic Regression Lasso Oversampling          0.712934    0.919618   \n",
      "                          Undersampling         0.712934    0.919775   \n",
      "                          SMOTE                 0.712934    0.916848   \n",
      "Decision Trees            Oversampling          0.526814    0.998459   \n",
      "                          Undersampling         0.940063    0.946231   \n",
      "                          SMOTE                 0.716088    0.992811   \n",
      "\n",
      "                                             Precision    Recall  F1 Score  \n",
      "Model                     Sampling Technique                                \n",
      "Logistic Regression       Oversampling        0.032749  0.712934  0.062621  \n",
      "                          Undersampling       0.032383  0.712934  0.061952  \n",
      "                          SMOTE               0.031706  0.712934  0.060712  \n",
      "Logistic Regression Lasso Oversampling        0.032749  0.712934  0.062621  \n",
      "                          Undersampling       0.032811  0.712934  0.062734  \n",
      "                          SMOTE               0.031693  0.712934  0.060687  \n",
      "Decision Trees            Oversampling        0.566102  0.526814  0.545752  \n",
      "                          Undersampling       0.062566  0.940063  0.117323  \n",
      "                          SMOTE               0.275485  0.716088  0.397897  \n"
     ]
    }
   ],
   "source": [
    "#遍历不同模型和重采样方法\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Logistic Regression Lasso': LogisticRegression(penalty='l1', solver='saga', max_iter=1000),\n",
    "    'Decision Trees': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Define the sampling techniques\n",
    "sampling_techniques = {\n",
    "    'Oversampling': RandomOverSampler(random_state=42),\n",
    "    'Undersampling': RandomUnderSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "# DataFrame to store the performance metrics\n",
    "metrics = ['Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1 Score']\n",
    "index = pd.MultiIndex.from_product([models.keys(), sampling_techniques.keys()], names=['Model', 'Sampling Technique'])\n",
    "df_performance = pd.DataFrame(index=index, columns=metrics)\n",
    "\n",
    "# Loop through each model and sampling technique, train, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    for technique_name, technique in sampling_techniques.items():\n",
    "        # Apply sampling technique\n",
    "        X_resampled, y_resampled = technique.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_holdout)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        sensitivity, specificity, precision, recall, f1 = evaluate_model(y_holdout, y_pred)\n",
    "        \n",
    "        # Store the performance metrics\n",
    "        df_performance.loc[(model_name, technique_name), :] = sensitivity, specificity, precision, recall, f1\n",
    "\n",
    "# Display the performance DataFrame\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "                                              Sensitivity  Specificity  \\\n",
      "Model                     Sampling Technique                             \n",
      "Logistic Regression       Oversampling           0.712934     0.919618   \n",
      "                          Undersampling          0.712934     0.918679   \n",
      "                          SMOTE                  0.712934     0.916884   \n",
      "Logistic Regression Lasso Oversampling           0.712934     0.919618   \n",
      "                          Undersampling          0.712934     0.919775   \n",
      "                          SMOTE                  0.712934     0.916848   \n",
      "Decision Trees            Oversampling           0.526814     0.998459   \n",
      "                          Undersampling          0.940063     0.946231   \n",
      "                          SMOTE                  0.716088     0.992811   \n",
      "\n",
      "                                              Precision    Recall  F1 Score  \n",
      "Model                     Sampling Technique                                 \n",
      "Logistic Regression       Oversampling         0.032749  0.712934  0.062621  \n",
      "                          Undersampling        0.032383  0.712934  0.061952  \n",
      "                          SMOTE                0.031706  0.712934  0.060712  \n",
      "Logistic Regression Lasso Oversampling         0.032749  0.712934  0.062621  \n",
      "                          Undersampling        0.032811  0.712934  0.062734  \n",
      "                          SMOTE                0.031693  0.712934  0.060687  \n",
      "Decision Trees            Oversampling         0.566102  0.526814  0.545752  \n",
      "                          Undersampling        0.062566  0.940063  0.117323  \n",
      "                          SMOTE                0.275485  0.716088  0.397897  \n",
      "\n",
      "Max Values by Sampling Technique:\n",
      "                    Sensitivity  Specificity  Precision    Recall  F1 Score\n",
      "Sampling Technique                                                         \n",
      "Oversampling           0.712934     0.998459   0.566102  0.712934  0.545752\n",
      "SMOTE                  0.716088     0.992811   0.275485  0.716088  0.397897\n",
      "Undersampling          0.940063     0.946231   0.062566  0.940063  0.117323\n",
      "\n",
      "Max Values by Model:\n",
      "                           Sensitivity  Specificity  Precision    Recall  \\\n",
      "Model                                                                      \n",
      "Decision Trees                0.940063     0.998459   0.566102  0.940063   \n",
      "Logistic Regression           0.712934     0.919618   0.032749  0.712934   \n",
      "Logistic Regression Lasso     0.712934     0.919775   0.032811  0.712934   \n",
      "\n",
      "                           F1 Score  \n",
      "Model                                \n",
      "Decision Trees             0.545752  \n",
      "Logistic Regression        0.062621  \n",
      "Logistic Regression Lasso  0.062734  \n"
     ]
    }
   ],
   "source": [
    "df_performance = df_performance.astype(float)\n",
    "\n",
    "# Calculate minimum values for each sampling technique across all models\n",
    "max_by_sampling = df_performance.groupby(level='Sampling Technique').max()\n",
    "\n",
    "# Calculate minimum values for each model across all sampling techniques\n",
    "max_by_model = df_performance.groupby(level='Model').max()\n",
    "\n",
    "# Display the performance DataFrame and minimum values\n",
    "print(\"Performance Metrics:\")\n",
    "print(df_performance)\n",
    "print(\"\\nMax Values by Sampling Technique:\")\n",
    "print(max_by_sampling)\n",
    "print(\"\\nMax Values by Model:\")\n",
    "print(max_by_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity (Recall): Decision Trees with Undersampling shows a remarkably high sensitivity, significantly outperforming other model-sampling combinations. High sensitivity is crucial if the cost of false negatives is high.\n",
    "\n",
    "Specificity: Decision Trees with Oversampling exhibits the highest specificity, which indicates its strength in correctly identifying the negatives. However, this comes with a trade-off in sensitivity.\n",
    "\n",
    "Precision: Decision Trees with Oversampling also has the highest precision, which means it has a lower false positive rate.\n",
    "\n",
    "F1 Score: The F1 Score balances the precision and recall, and we can see that Decision Trees with Oversampling have the highest F1 score, indicating a balanced performance between precision and recall.\n",
    "\n",
    "Given these observations, the Decision Trees with Oversampling seem to perform best overall based on the F1 Score, which is a harmonic mean of precision and recall. \n",
    "\n",
    "But there is not a resampling method that outperformance others when models change. SMOTE works better at  recall and sensitivity, oversampling works better at Specificity, precision and F-1 score, while undersampling works better at sensitivity and recall.\n",
    "\n",
    "All in all, I prefer the Decision Trees with Oversampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
